{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04e87af77d3442bd8bad4a6cd8d811ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0129f1aec05d44a5be4b9becd5b054d5",
              "IPY_MODEL_6472f55a256e45a0b15982d72e083f8f",
              "IPY_MODEL_5e4bc393b13d47de85c4e5c8a340c21a"
            ],
            "layout": "IPY_MODEL_ca69c8de599045ac8a423f229b930b41"
          }
        },
        "0129f1aec05d44a5be4b9becd5b054d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fb9653bd54c41cd851fbf8402130ebe",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_76396870f1f54e2fb686f52450597704",
            "value": "Map:â€‡100%"
          }
        },
        "6472f55a256e45a0b15982d72e083f8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f61556f36cb45929167e126c369416f",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d76b6403e7074b2c8ab031f3957cd0b2",
            "value": 1000
          }
        },
        "5e4bc393b13d47de85c4e5c8a340c21a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c449ecd86d64d49bc15998ced7931c5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cc232f068c1f4137aa0cf8fd708ac27a",
            "value": "â€‡1000/1000â€‡[00:00&lt;00:00,â€‡13754.61â€‡examples/s]"
          }
        },
        "ca69c8de599045ac8a423f229b930b41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fb9653bd54c41cd851fbf8402130ebe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76396870f1f54e2fb686f52450597704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f61556f36cb45929167e126c369416f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d76b6403e7074b2c8ab031f3957cd0b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c449ecd86d64d49bc15998ced7931c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc232f068c1f4137aa0cf8fd708ac27a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c40246fc4c64410b91d4e27e44f1ae93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45f465938d4c41e78570dad0669abfed",
              "IPY_MODEL_cae0eaba50274ce2a4d089c25c6b1a95",
              "IPY_MODEL_724da24f92794705aced395f41207e27"
            ],
            "layout": "IPY_MODEL_88f4895c72f4488fae557520918162cd"
          }
        },
        "45f465938d4c41e78570dad0669abfed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7a00e41c8b2427c906b3a7846df846d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6f09609f0fd24463adcffe40833db516",
            "value": "Filter:â€‡100%"
          }
        },
        "cae0eaba50274ce2a4d089c25c6b1a95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbea2a5097064660bb12f2b40f26e14a",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b4e6d880e6f4d389dc0129c28e0820f",
            "value": 1000
          }
        },
        "724da24f92794705aced395f41207e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a1c82e8847f43979b5b7db6566aad42",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7dabb47a507849a5814e83ab824cf2dd",
            "value": "â€‡1000/1000â€‡[00:00&lt;00:00,â€‡22585.36â€‡examples/s]"
          }
        },
        "88f4895c72f4488fae557520918162cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7a00e41c8b2427c906b3a7846df846d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f09609f0fd24463adcffe40833db516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbea2a5097064660bb12f2b40f26e14a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b4e6d880e6f4d389dc0129c28e0820f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a1c82e8847f43979b5b7db6566aad42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dabb47a507849a5814e83ab824cf2dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnb-I109qHVC",
        "outputId": "c9d6d2ab-1e98-4d51-ab85-a0fc666a87cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Install complete. Now do: Runtime â†’ Restart runtimeâ€¦  then run the next cell Iâ€™ll send.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Cell A â€” Set up a stable Unsloth + TRL GRPO environment for Colab T4 (CUDA 12.4).\n",
        "\n",
        "What this cell does:\n",
        "1) Upgrades pip.\n",
        "2) Installs the *paired* CUDA 12.4 wheels (torch==2.6.0, torchvision==0.21.0, torchaudio==2.6.0).\n",
        "3) Installs Unsloth via the *correct extras* for (cu124, torch 2.6) so Zoo matches Torch/CUDA.\n",
        "4) Pins training stack versions that are known-good for GRPO with Unsloth:\n",
        "   transformers==4.56.1, trl==0.23.0, accelerate>=1.0.1, peft>=0.13.2, bitsandbytes, datasets==4.3.0, etc.\n",
        "5) Prints a reminder to manually restart the runtime so the new wheels are loaded.\n",
        "\n",
        "Notes:\n",
        "- We intentionally avoid importing torch/transformers in this cell to prevent in-memory version confusion.\n",
        "- If you previously ran other notebooks in the same runtime, do Runtime â†’ Factory reset runtimeâ€¦ before this.\n",
        "\"\"\"\n",
        "\n",
        "import sys, subprocess\n",
        "\n",
        "PIP = [sys.executable, \"-m\", \"pip\"]\n",
        "\n",
        "# 1) Fresh pip\n",
        "subprocess.check_call(PIP + [\"install\", \"-U\", \"pip\"])\n",
        "\n",
        "# 2) PyTorch + CUDA 12.4 wheels (paired versions avoid torchvision::nms errors)\n",
        "subprocess.check_call(PIP + [\n",
        "    \"install\", \"-U\", \"--quiet\", \"--no-cache-dir\",\n",
        "    \"torch==2.6.0+cu124\", \"torchvision==0.21.0+cu124\", \"torchaudio==2.6.0+cu124\",\n",
        "    \"--index-url\", \"https://download.pytorch.org/whl/cu124\"\n",
        "])\n",
        "\n",
        "# 3) Unsloth for this exact (CUDA, Torch) pair via extras (pulls matching zoo)\n",
        "subprocess.check_call([\n",
        "    \"bash\", \"-lc\",\n",
        "    'pip install -U \"unsloth[cu124-torch260] @ git+https://github.com/unslothai/unsloth.git\"'\n",
        "])\n",
        "\n",
        "# 4) Core training libs (versions aligned for TRL + Unsloth)\n",
        "subprocess.check_call(PIP + [\n",
        "    \"install\", \"-U\", \"--quiet\", \"--no-cache-dir\",\n",
        "    \"transformers==4.56.1\",\n",
        "    \"trl==0.23.0\",\n",
        "    \"accelerate>=1.0.1\",\n",
        "    \"peft>=0.13.2\",\n",
        "    \"bitsandbytes\",\n",
        "    \"datasets==4.3.0\",\n",
        "    \"sentencepiece\",\n",
        "    \"protobuf>=5.28.3\",\n",
        "    \"huggingface_hub>=0.24.6\",\n",
        "    \"hf_transfer\",\n",
        "])\n",
        "\n",
        "print(\"\\nâœ… Install complete. Now do: Runtime â†’ Restart runtimeâ€¦  then run the next cell Iâ€™ll send.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell B â€” Safe imports & environment sanity for GRPO\n",
        "\n",
        "What this cell does:\n",
        "1) Sets Unslothâ€™s safe flags for a stable first import.\n",
        "2) Imports `unsloth` FIRST (required for its optimizations), then Hugging Face libs.\n",
        "3) Prints concise environment & GPU info.\n",
        "4) Asserts that TRLâ€™s `GRPOTrainer` is available and that Transformers exposes chat templating,\n",
        "   so we can format reasoning prompts safely.\n",
        "\n",
        "If this cell prints the final âœ… line, weâ€™re ready to load data and define rewards next.\n",
        "\"\"\"\n",
        "\n",
        "import os, platform, importlib\n",
        "\n",
        "# 1) Conservative flags for a clean first import (can be relaxed later)\n",
        "os.environ.setdefault(\"UNSLOTH_COMPILE_DISABLE\", \"1\")\n",
        "os.environ.setdefault(\"UNSLOTH_DISABLE_FAST_GENERATION\", \"1\")\n",
        "os.environ.setdefault(\"HF_HUB_DISABLE_PROGRESS_BARS\", \"1\")\n",
        "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
        "\n",
        "# 2) Import order: Unsloth FIRST\n",
        "import unsloth\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "\n",
        "# 3) Hugging Face stack\n",
        "import torch, transformers, trl, datasets\n",
        "\n",
        "# 4) Minimal sanity checks\n",
        "# - TRL GRPO available?\n",
        "try:\n",
        "    from trl import GRPOTrainer  # noqa: F401\n",
        "    grpo_ok = True\n",
        "except Exception as e:\n",
        "    grpo_ok = False\n",
        "    print(\"GRPO import error:\", repr(e))\n",
        "\n",
        "# - Chat templating available?\n",
        "templating_ok = hasattr(transformers.PreTrainedTokenizerBase, \"apply_chat_template\")\n",
        "\n",
        "# - GPU info\n",
        "cuda_ok = torch.cuda.is_available()\n",
        "gpu_name = torch.cuda.get_device_name(0) if cuda_ok else \"CPU\"\n",
        "cc = torch.cuda.get_device_capability(0) if cuda_ok else (\"-\", \"-\")\n",
        "\n",
        "print(\"Python       :\", platform.python_version())\n",
        "print(\"Torch        :\", torch.__version__, \"| CUDA:\", torch.version.cuda, \"| CUDA available:\", cuda_ok)\n",
        "print(\"GPU          :\", gpu_name, \"| CC:\", cc)\n",
        "print(\"Transformers :\", transformers.__version__)\n",
        "print(\"TRL          :\", trl.__version__, \"| GRPO available:\", grpo_ok)\n",
        "print(\"Datasets     :\", datasets.__version__)\n",
        "print(\"Unsloth      :\", getattr(unsloth, \"__version__\", \"git\"))\n",
        "print(\"Chat templating available:\", templating_ok)\n",
        "\n",
        "assert grpo_ok, \"TRLâ€™s GRPOTrainer not found â€” please re-check TRL install/pin.\"\n",
        "assert templating_ok, \"Transformers chat templating missing â€” please re-check Transformers version.\"\n",
        "\n",
        "print(\"âœ… Imports & sanity checks passed. Ready for dataset + rewards.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYtD0UBZqSru",
        "outputId": "ad91f3e8-da5a-4539-8621-aa4f951b8387"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "Python       : 3.12.12\n",
            "Torch        : 2.6.0+cu124 | CUDA: 12.4 | CUDA available: True\n",
            "GPU          : Tesla T4 | CC: (7, 5)\n",
            "Transformers : 4.56.1\n",
            "TRL          : 0.23.0 | GRPO available: True\n",
            "Datasets     : 4.3.0\n",
            "Unsloth      : 2025.11.3\n",
            "Chat templating available: True\n",
            "âœ… Imports & sanity checks passed. Ready for dataset + rewards.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell C â€” Prepare the GRPO dataset (GSM8K â†’ prompts + ground_truth) â€” fixed\n",
        "\n",
        "What this cell does:\n",
        "1) Loads a true `datasets.Dataset` slice using the split slicer (NOT `dataset[:N]`, which returns a dict).\n",
        "2) Extracts the numeric ground-truth answer from GSM8K's 'answer' (after the '####' marker).\n",
        "3) Builds a conversational `prompt` with a system format instruction enforcing:\n",
        "      <reasoning>...</reasoning><answer>...</answer>\n",
        "4) Filters rows missing ground-truth and prints 2 examples.\n",
        "\n",
        "Why this fixes your error:\n",
        "- `dataset[:N]` returns a dict-of-lists, so `.map(...)` fails. Using\n",
        "  `split=\"train[:N]\"` (or `.select(range(N))`) preserves the `Dataset` API (supports `.map`, `.filter`).\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\".strip()\n",
        "\n",
        "def extract_hash_answer(text: str):\n",
        "    # GSM8K final answer appears after '####'\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    return text.split(\"####\", 1)[1].strip()\n",
        "\n",
        "_num_pat = re.compile(r\"^[\\s]*([-+]?\\d[\\d,]*([.]\\d+)?)\\s*$\")\n",
        "\n",
        "def normalize_number(s: str):\n",
        "    if s is None:\n",
        "        return None\n",
        "    m = _num_pat.match(s.strip())\n",
        "    s = (m.group(1) if m else s).replace(\",\", \"\").strip()\n",
        "    return s\n",
        "\n",
        "# 1) Load a Dataset slice (keeps Dataset API intact)\n",
        "N_ROWS = 1000  # bump later\n",
        "train = load_dataset(\"openai/gsm8k\", \"main\", split=f\"train[:{N_ROWS}]\")\n",
        "\n",
        "# 2) Map â†’ add prompt + ground_truth\n",
        "def to_prompt_row(row):\n",
        "    gt = normalize_number(extract_hash_answer(row[\"answer\"]))\n",
        "    return {\n",
        "        \"prompt\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\",   \"content\": row[\"question\"]},\n",
        "        ],\n",
        "        \"ground_truth\": gt,\n",
        "        \"question\": row[\"question\"],\n",
        "    }\n",
        "\n",
        "train = train.map(to_prompt_row)\n",
        "\n",
        "# 3) Keep only the columns we need\n",
        "keep_cols = [\"prompt\", \"ground_truth\", \"question\"]\n",
        "drop_cols = [c for c in train.column_names if c not in keep_cols]\n",
        "if drop_cols:\n",
        "    train = train.remove_columns(drop_cols)\n",
        "\n",
        "# 4) Filter rows without ground_truth (rare)\n",
        "train = train.filter(lambda r: r[\"ground_truth\"] is not None)\n",
        "\n",
        "# 5) Peek\n",
        "def compact(msgs, limit=140):\n",
        "    parts = []\n",
        "    for m in msgs:\n",
        "        s = f\"{m['role'].upper()}: {m['content']}\"\n",
        "        parts.append(s if len(s) <= limit else s[:limit] + \" â€¦\")\n",
        "    return \" | \".join(parts)\n",
        "\n",
        "print(\"Rows:\", len(train))\n",
        "print(\"Columns:\", train.column_names)\n",
        "print(\"\\nExample 1:\")\n",
        "print(compact(train[0][\"prompt\"]))\n",
        "print(\"ground_truth:\", train[0][\"ground_truth\"])\n",
        "if len(train) > 1:\n",
        "    print(\"\\nExample 2:\")\n",
        "    print(compact(train[1][\"prompt\"]))\n",
        "    print(\"ground_truth:\", train[1][\"ground_truth\"])\n",
        "\n",
        "print(\"\\nâœ… GSM8K prepared with conversational prompts and normalized numeric ground_truth.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518,
          "referenced_widgets": [
            "04e87af77d3442bd8bad4a6cd8d811ff",
            "0129f1aec05d44a5be4b9becd5b054d5",
            "6472f55a256e45a0b15982d72e083f8f",
            "5e4bc393b13d47de85c4e5c8a340c21a",
            "ca69c8de599045ac8a423f229b930b41",
            "0fb9653bd54c41cd851fbf8402130ebe",
            "76396870f1f54e2fb686f52450597704",
            "9f61556f36cb45929167e126c369416f",
            "d76b6403e7074b2c8ab031f3957cd0b2",
            "1c449ecd86d64d49bc15998ced7931c5",
            "cc232f068c1f4137aa0cf8fd708ac27a",
            "c40246fc4c64410b91d4e27e44f1ae93",
            "45f465938d4c41e78570dad0669abfed",
            "cae0eaba50274ce2a4d089c25c6b1a95",
            "724da24f92794705aced395f41207e27",
            "88f4895c72f4488fae557520918162cd",
            "a7a00e41c8b2427c906b3a7846df846d",
            "6f09609f0fd24463adcffe40833db516",
            "bbea2a5097064660bb12f2b40f26e14a",
            "9b4e6d880e6f4d389dc0129c28e0820f",
            "5a1c82e8847f43979b5b7db6566aad42",
            "7dabb47a507849a5814e83ab824cf2dd"
          ]
        },
        "id": "ae5ryF8ssvYa",
        "outputId": "a815b23d-3e7c-4339-890e-9530c987960e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04e87af77d3442bd8bad4a6cd8d811ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c40246fc4c64410b91d4e27e44f1ae93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 1000\n",
            "Columns: ['question', 'prompt', 'ground_truth']\n",
            "\n",
            "Example 1:\n",
            "SYSTEM: Respond in the following format:\n",
            "<reasoning>\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "...\n",
            "</answer> | USER: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altoget â€¦\n",
            "ground_truth: 72\n",
            "\n",
            "Example 2:\n",
            "SYSTEM: Respond in the following format:\n",
            "<reasoning>\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "...\n",
            "</answer> | USER: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
            "ground_truth: 10\n",
            "\n",
            "âœ… GSM8K prepared with conversational prompts and normalized numeric ground_truth.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell D â€” Reward functions for GRPO (format + exact answer)\n",
        "\n",
        "What this cell does:\n",
        "1) Implements two TRL-compatible reward functions:\n",
        "   - format_reward(completions, **kwargs) â†’ 1.0 if completion matches\n",
        "     `<reasoning>â€¦</reasoning><answer>â€¦</answer>`, else 0.0.\n",
        "   - exact_answer_reward(completions, ground_truth, **kwargs) â†’ 1.0 if the\n",
        "     extracted <answer> equals the normalized numeric ground_truth, else 0.0.\n",
        "\n",
        "2) Includes a tiny, fast unit test to prove both functions return lists of floats\n",
        "   and have the right shape for GRPOTrainer.\n",
        "\n",
        "Why this matches references:\n",
        "- TRL reward functions receive a list of completions (each completion is a single\n",
        "  assistant message with a \"content\" field) and must return list[float]. (See TRL docs)\n",
        "- GSM8Kâ€™s answer is the number after the `####` marker; we already prepared the\n",
        "  dataset to expose a normalized `ground_truth` column. (See GSM8K dataset card)\n",
        "- Unslothâ€™s GRPO tutorial enforces reasoning+answer tags and uses accuracy-like rewards.\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Precompile a strict \"reasoning+answer\" pattern, allowing whitespace/newlines between tags\n",
        "_REASONING_ANSWER_RE = re.compile(\n",
        "    r\"^\\s*<reasoning>[\\s\\S]+?</reasoning>\\s*<answer>\\s*([\\s\\S]+?)\\s*</answer>\\s*$\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "_NUM_NORM_RE = re.compile(r\"^[\\s]*([-+]?\\d[\\d,]*([.]\\d+)?)\\s*$\")\n",
        "\n",
        "def _get_text_from_completion(c: List[Dict[str, str]]) -> str:\n",
        "    \"\"\"\n",
        "    Each completion is a list with one dict {\"content\": \"...\"} (TRL convention).\n",
        "    Be lenient if a model returns role+content.\n",
        "    \"\"\"\n",
        "    if not c:\n",
        "        return \"\"\n",
        "    msg = c[0]\n",
        "    return msg.get(\"content\", \"\") if isinstance(msg, dict) else str(msg)\n",
        "\n",
        "def _normalize_number(s: str | None) -> str | None:\n",
        "    if s is None:\n",
        "        return None\n",
        "    s = s.strip()\n",
        "    m = _NUM_NORM_RE.match(s)\n",
        "    if m:\n",
        "        s = m.group(1)\n",
        "    return s.replace(\",\", \"\").strip()\n",
        "\n",
        "def format_reward(completions: List[List[Dict[str, str]]], **kwargs: Any) -> List[float]:\n",
        "    \"\"\"\n",
        "    Reward = 1.0 if completion matches:\n",
        "      <reasoning>...</reasoning><answer>...</answer>\n",
        "    else 0.0\n",
        "    \"\"\"\n",
        "    rewards: List[float] = []\n",
        "    for c in completions:\n",
        "        text = _get_text_from_completion(c)\n",
        "        rewards.append(1.0 if _REASONING_ANSWER_RE.match(text) else 0.0)\n",
        "    return rewards\n",
        "\n",
        "def exact_answer_reward(\n",
        "    completions: List[List[Dict[str, str]]],\n",
        "    ground_truth: List[str] | None = None,\n",
        "    **kwargs: Any,\n",
        ") -> List[float]:\n",
        "    \"\"\"\n",
        "    Reward = 1.0 if extracted <answer> equals ground_truth (normalized), else 0.0.\n",
        "    `ground_truth` is expected to come from the batch (our dataset column).\n",
        "    \"\"\"\n",
        "    gt_list = ground_truth or kwargs.get(\"ground_truth\") or []\n",
        "    out: List[float] = []\n",
        "    for i, c in enumerate(completions):\n",
        "        text = _get_text_from_completion(c)\n",
        "        m = _REASONING_ANSWER_RE.match(text)\n",
        "        if not m:\n",
        "            out.append(0.0)\n",
        "            continue\n",
        "        pred = _normalize_number(m.group(1))\n",
        "        gold = _normalize_number(gt_list[i] if i < len(gt_list) else None)\n",
        "        out.append(1.0 if (pred is not None and gold is not None and pred == gold) else 0.0)\n",
        "    return out\n",
        "\n",
        "# --------- Tiny unit tests (run fast) ---------\n",
        "_test_completions = [\n",
        "    [{\"content\": \"<reasoning>\\nsteps\\n</reasoning>\\n<answer>\\n72\\n</answer>\"}],\n",
        "    [{\"content\": \"<reasoning>\\noops missing answer tag\\n</reasoning>\"}],\n",
        "    [{\"content\": \"<reasoning>ok</reasoning><answer>10</answer>\"}],\n",
        "]\n",
        "_test_gt = [\"72\", \"10\", \"10\"]\n",
        "\n",
        "fr = format_reward(_test_completions)\n",
        "ar = exact_answer_reward(_test_completions, ground_truth=_test_gt)\n",
        "\n",
        "print(\"format_reward â†’\", fr)  # expect [1.0, 0.0, 1.0]\n",
        "print(\"exact_answer_reward â†’\", ar)  # expect [1.0, 0.0, 1.0]\n",
        "assert all(isinstance(x, float) for x in fr) and len(fr) == len(_test_completions)\n",
        "assert all(isinstance(x, float) for x in ar) and len(ar) == len(_test_completions)\n",
        "\n",
        "print(\"âœ… Rewards ready: TRL-compatible signatures and shapes.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM1rMn60uCCo",
        "outputId": "36494b3c-e3b3-4bac-ff9f-793108b8fb78"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "format_reward â†’ [1.0, 0.0, 1.0]\n",
            "exact_answer_reward â†’ [1.0, 0.0, 1.0]\n",
            "âœ… Rewards ready: TRL-compatible signatures and shapes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell E â€” Model setup for GRPO (4-bit Qwen2.5-1.5B-Instruct + LoRA + chat template) + smoke generation\n",
        "\n",
        "What this cell does:\n",
        "1) Loads a small, T4-friendly 4-bit instruct model: unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit.\n",
        "2) Applies LoRA adapters (memory-efficient training).\n",
        "3) Ensures a proper chat template is attached (Qwen2.5 Instruct models usually ship one;\n",
        "   we attach Unsloth's 'qwen25' template if missing).\n",
        "4) Runs a quick smoke generation that enforces the <reasoning>...</reasoning><answer>...</answer> format.\n",
        "\n",
        "Why this setup:\n",
        "- Qwen2.5-1.5B-Instruct-bnb-4bit is lightweight enough to sample multiple completions per prompt on a T4.\n",
        "- Instruct variants include chat templates; if not present, we attach Unsloth's 'qwen25' template.\n",
        "- LoRA keeps only ~1% of parameters trainable, ideal for GRPO on a single T4.\n",
        "\"\"\"\n",
        "\n",
        "import torch, math\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from transformers import TextStreamer\n",
        "\n",
        "MODEL_NAME   = \"unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit\"\n",
        "MAX_SEQ_LEN  = 2048\n",
        "use_bf16     = is_bfloat16_supported()\n",
        "dtype        = torch.bfloat16 if use_bf16 else torch.float16\n",
        "\n",
        "# 1) Load 4-bit base + tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name      = MODEL_NAME,\n",
        "    max_seq_length  = MAX_SEQ_LEN,\n",
        "    load_in_4bit    = True,\n",
        "    dtype           = dtype,\n",
        ")\n",
        "\n",
        "# 2) Tokenizer padding sanity\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# 3) Ensure chat template exists (Qwen2.5 Instruct usually has one; attach if missing)\n",
        "if not getattr(tokenizer, \"chat_template\", None):\n",
        "    from unsloth.chat_templates import get_chat_template\n",
        "    tokenizer = get_chat_template(tokenizer, chat_template=\"qwen25\")\n",
        "print(\"Chat template attached:\", bool(getattr(tokenizer, \"chat_template\", None)))\n",
        "\n",
        "# 4) Attach LoRA adapters (Qwen/Llama-style projection names)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=42,\n",
        "    max_seq_length=MAX_SEQ_LEN,\n",
        ")\n",
        "\n",
        "# ---- Diagnostics ----\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total     = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Loaded: {MODEL_NAME}\")\n",
        "print(f\"Max seq len: {MAX_SEQ_LEN} | Dtype: {dtype} | BF16 supported: {use_bf16}\")\n",
        "print(f\"Params: {trainable:,} trainable / {total:,} total (~{100*trainable/total:.2f}% trainable)\")\n",
        "print(\"Tokenizer pad_token_id:\", tokenizer.pad_token_id, \"| padding_side:\", tokenizer.padding_side)\n",
        "\n",
        "# 5) Quick smoke generation to verify formatting & template\n",
        "def _to_device(batch, device):\n",
        "    # Accept BatchEncoding (dict-like) or Tensor â†’ mapping\n",
        "    from collections.abc import Mapping\n",
        "    if isinstance(batch, Mapping):\n",
        "        return {k: (v.to(device) if hasattr(v, \"to\") else v) for k, v in batch.items()}\n",
        "    import torch\n",
        "    if torch.is_tensor(batch):\n",
        "        return {\"input_ids\": batch.to(device), \"attention_mask\": torch.ones_like(batch, device=device)}\n",
        "    raise TypeError(f\"Unexpected inputs type: {type(batch)}\")\n",
        "\n",
        "def chat(messages, max_new_tokens=160, temperature=0.8, top_p=0.95):\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_dict=True,\n",
        "        tokenize=True,\n",
        "    )\n",
        "    inputs = _to_device(inputs, model.device)\n",
        "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "    from torch import no_grad\n",
        "    with no_grad(), torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    new_tokens = outputs[0, prompt_len:]\n",
        "    return tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"Respond in the following format:\\n\"\n",
        "    \"<reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\"\n",
        ")\n",
        "demo_q = \"If a book costs $8 and a pen costs $2, how much for 3 books and 4 pens?\"\n",
        "\n",
        "print(\"\\n=== Smoke generation ===\")\n",
        "print(chat([\n",
        "    {\"role\":\"system\",\"content\": SYSTEM_PROMPT},\n",
        "    {\"role\":\"user\",\"content\": demo_q},\n",
        "])[:600])\n",
        "\n",
        "print(\"\\nâœ… Model, LoRA and chat template are ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyUd2SlAuml4",
        "outputId": "18651923-0199-426f-90f1-d8806efa5ae2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.11.3: Fast Qwen2 patching. Transformers: 4.56.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Chat template attached: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.11.3 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit\n",
            "Max seq len: 2048 | Dtype: torch.float16 | BF16 supported: False\n",
            "Params: 18,464,768 trainable / 907,081,216 total (~2.04% trainable)\n",
            "Tokenizer pad_token_id: 151654 | padding_side: right\n",
            "\n",
            "=== Smoke generation ===\n",
            "To calculate the total cost of 3 books and 4 pens, we need to multiply the cost of each item by the quantity and then add them together. \n",
            "\n",
            "The calculation is as follows:\n",
            "\n",
            "\\( (3 \\times \\$8) + (4 \\times \\$2) = \\$24 + \\$8 = \\$32 \\).\n",
            "\n",
            "So, the total cost would be \\( \\$32 \\).\n",
            "\n",
            "âœ… Model, LoRA and chat template are ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell F â€” Configure GRPO (T4-friendly) and build the GRPOTrainer\n",
        "\n",
        "What this cell does:\n",
        "1) Creates a GRPOConfig tuned for Colab T4:\n",
        "   - num_generations=3  (sample 3 completions per prompt)\n",
        "   - modest lengths (max_prompt_length=512, max_completion_length=256)\n",
        "   - small batch with gradient accumulation (fits T4)\n",
        "   - loss_type=\"dapo\" and beta=0.0 (common, length-bias aware; KL off by default)\n",
        "   - temperature/top_p for diversity during GRPO sampling\n",
        "   - 8-bit AdamW optimizer\n",
        "\n",
        "2) Instantiates GRPOTrainer with:\n",
        "   - our LoRA model + tokenizer as processing_class (applies chat template)\n",
        "   - BOTH reward functions: format_reward + exact_answer_reward\n",
        "   - the GSM8K prompt dataset from earlier (columns: prompt, ground_truth)\n",
        "\n",
        "3) Prints a short summary so we can sanity-check shapes & knobs before training.\n",
        "\n",
        "Why these choices (backed by docs):\n",
        "- TRLâ€™s GRPO docs show using `GRPOConfig` (num_generations, lengths, loss_type, beta).\n",
        "- DAPO/Î²=0.0 is a recommended starting point; scale rewards can be changed later.\n",
        "- Passing `processing_class=tokenizer` is the documented way to enable chat templating in GRPO.\n",
        "\"\"\"\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "# --- T4-friendly GRPO defaults (you can relax later) ---\n",
        "USE_BF16 = False  # our runtime reports BF16 False; keep fp16\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=\"grpo_qwen15b_gsm8k_runs\",\n",
        "    learning_rate=5e-6,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=6,\n",
        "    max_steps=100,                     # quick loop check; scale later\n",
        "    logging_steps=5,\n",
        "    save_steps=1000,                   # effectively off for this short run\n",
        "    report_to=\"none\",\n",
        "\n",
        "    # Generation (GRPO sampling) knobs\n",
        "    num_generations=3,                 # completions per prompt (G)\n",
        "    max_prompt_length=512,\n",
        "    max_completion_length=256,\n",
        "    temperature=1.0,\n",
        "    top_p=0.95,\n",
        "    top_k=0,\n",
        "\n",
        "    # GRPO objective & shaping\n",
        "    loss_type=\"dapo\",                  # length-bias mitigation\n",
        "    beta=0.0,                          # KL off (per TRL defaults/notes)\n",
        "    scale_rewards=\"batch\",             # robust shaping across batch (optional)\n",
        "\n",
        "    # Precision / optimizer\n",
        "    fp16=not USE_BF16,\n",
        "    bf16=USE_BF16,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        ")\n",
        "\n",
        "# Build trainer â€” TRL will:\n",
        "#  * sample G completions per prompt\n",
        "#  * call our reward functions\n",
        "#  * apply DAPO objective over generated tokens\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,                                   # LoRA model from Cell E\n",
        "    processing_class=tokenizer,                    # ensures chat templating\n",
        "    args=training_args,\n",
        "    train_dataset=train,                           # Dataset with 'prompt' + 'ground_truth'\n",
        "    reward_funcs=[format_reward, exact_answer_reward],\n",
        ")\n",
        "\n",
        "# --- Summarize critical bits ---\n",
        "print(\"Trainer ready.\")\n",
        "print(\"num_generations       :\", trainer.num_generations)\n",
        "print(\"max_prompt_length     :\", trainer.max_prompt_length)\n",
        "print(\"max_completion_length :\", trainer.max_completion_length)\n",
        "print(\"loss_type / beta      :\", training_args.loss_type, training_args.beta)\n",
        "print(\"batch / grad_accum    :\", training_args.per_device_train_batch_size, training_args.gradient_accumulation_steps)\n",
        "print(\"dtype(fp16/bf16)      :\", training_args.fp16, training_args.bf16)\n",
        "print(\"âœ… GRPOTrainer constructed. Next cell will run a short train (100 steps).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_1LD1H4u55Q",
        "outputId": "7535846a-ccf6-4560-8693-40d4cb640de6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: The DAPO paper recommends `mask_truncated_completions = True` - we will set it.\n",
            "Unsloth: The DAPO paper recommends `epsilon_high = 0.28` - we will set it.\n",
            "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
            "We will change the batch size of 1 to the `num_generations` of 3\n",
            "Trainer ready.\n",
            "num_generations       : 3\n",
            "max_prompt_length     : 512\n",
            "max_completion_length : 256\n",
            "loss_type / beta      : dapo 0.0\n",
            "batch / grad_accum    : 3 6\n",
            "dtype(fp16/bf16)      : True False\n",
            "âœ… GRPOTrainer constructed. Next cell will run a short train (100 steps).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell G â€” Run a short GRPO training loop (100 steps)\n",
        "\n",
        "What this cell does:\n",
        "1) Sets a seed for reproducibility of sampling and rewards.\n",
        "2) Calls `trainer.train()` to:\n",
        "   - sample `num_generations` completions per prompt,\n",
        "   - compute both rewards (format + exact-answer),\n",
        "   - optimize with the DAPO loss (reference-free, beta=0.0),\n",
        "   - log progress every few steps.\n",
        "\n",
        "Notes:\n",
        "- Settings and behavior follow Unslothâ€™s GRPO tutorial and TRLâ€™s GRPOTrainer API.\n",
        "- You should see logs per a few steps; on completion, a TrainOutput summary prints.\n",
        "\"\"\"\n",
        "from transformers import set_seed\n",
        "import time\n",
        "\n",
        "set_seed(42)\n",
        "t0 = time.time()\n",
        "train_output = trainer.train()\n",
        "t1 = time.time()\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "print(train_output)\n",
        "print(f\"Wall clock (s): {t1 - t0:.1f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "nvL5xDe0v5pN",
        "outputId": "83baf9f6-471c-49c0-804f-692ca9ee55b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 3 | Gradient accumulation steps = 6\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (3 x 6 x 1) = 18\n",
            " \"-____-\"     Trainable parameters = 18,464,768 of 1,562,179,072 (1.18% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 59:33, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completions / mean_length</th>\n",
              "      <th>completions / min_length</th>\n",
              "      <th>completions / max_length</th>\n",
              "      <th>completions / clipped_ratio</th>\n",
              "      <th>completions / mean_terminated_length</th>\n",
              "      <th>completions / min_terminated_length</th>\n",
              "      <th>completions / max_terminated_length</th>\n",
              "      <th>sampling / sampling_logp_difference / mean</th>\n",
              "      <th>sampling / sampling_logp_difference / max</th>\n",
              "      <th>sampling / importance_sampling_ratio / min</th>\n",
              "      <th>sampling / importance_sampling_ratio / mean</th>\n",
              "      <th>sampling / importance_sampling_ratio / max</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / format_reward / mean</th>\n",
              "      <th>rewards / format_reward / std</th>\n",
              "      <th>rewards / exact_answer_reward / mean</th>\n",
              "      <th>rewards / exact_answer_reward / std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.044444</td>\n",
              "      <td>0.188562</td>\n",
              "      <td>188.255557</td>\n",
              "      <td>86.000000</td>\n",
              "      <td>256.000000</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>161.884250</td>\n",
              "      <td>86.000000</td>\n",
              "      <td>237.200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.044444</td>\n",
              "      <td>0.188562</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>185.233334</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>250.200000</td>\n",
              "      <td>0.155556</td>\n",
              "      <td>172.005865</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>242.400000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.064676</td>\n",
              "      <td>198.055554</td>\n",
              "      <td>106.400000</td>\n",
              "      <td>256.000000</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>177.341550</td>\n",
              "      <td>106.400000</td>\n",
              "      <td>242.800000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.064676</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.044444</td>\n",
              "      <td>0.188562</td>\n",
              "      <td>186.044446</td>\n",
              "      <td>103.000000</td>\n",
              "      <td>256.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>168.653580</td>\n",
              "      <td>103.000000</td>\n",
              "      <td>247.000000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.141421</td>\n",
              "      <td>0.011111</td>\n",
              "      <td>0.047140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.144444</td>\n",
              "      <td>0.383523</td>\n",
              "      <td>186.888889</td>\n",
              "      <td>88.600000</td>\n",
              "      <td>256.000000</td>\n",
              "      <td>0.211111</td>\n",
              "      <td>168.556992</td>\n",
              "      <td>88.600000</td>\n",
              "      <td>237.600000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.274072</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.141421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.122222</td>\n",
              "      <td>0.420813</td>\n",
              "      <td>201.655557</td>\n",
              "      <td>91.400000</td>\n",
              "      <td>256.000000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>177.401416</td>\n",
              "      <td>91.400000</td>\n",
              "      <td>241.600000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.223633</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.206098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.572613</td>\n",
              "      <td>185.611115</td>\n",
              "      <td>90.400000</td>\n",
              "      <td>256.000000</td>\n",
              "      <td>0.244444</td>\n",
              "      <td>162.873083</td>\n",
              "      <td>90.400000</td>\n",
              "      <td>245.800000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.233333</td>\n",
              "      <td>0.421678</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.188513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.673521</td>\n",
              "      <td>162.444443</td>\n",
              "      <td>69.400000</td>\n",
              "      <td>256.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>139.336130</td>\n",
              "      <td>69.400000</td>\n",
              "      <td>240.400000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.344444</td>\n",
              "      <td>0.479759</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.300330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.852407</td>\n",
              "      <td>171.000000</td>\n",
              "      <td>78.200000</td>\n",
              "      <td>256.000000</td>\n",
              "      <td>0.177778</td>\n",
              "      <td>154.922461</td>\n",
              "      <td>78.200000</td>\n",
              "      <td>236.800000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.577778</td>\n",
              "      <td>0.469853</td>\n",
              "      <td>0.322222</td>\n",
              "      <td>0.478710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.966667</td>\n",
              "      <td>0.769931</td>\n",
              "      <td>176.266669</td>\n",
              "      <td>82.800000</td>\n",
              "      <td>256.000000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>159.794537</td>\n",
              "      <td>82.800000</td>\n",
              "      <td>249.000000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.688889</td>\n",
              "      <td>0.463208</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.451773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.044444</td>\n",
              "      <td>0.734844</td>\n",
              "      <td>165.122226</td>\n",
              "      <td>84.400000</td>\n",
              "      <td>252.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>155.569727</td>\n",
              "      <td>84.400000</td>\n",
              "      <td>243.600000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.744444</td>\n",
              "      <td>0.436441</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.441612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.155556</td>\n",
              "      <td>0.617866</td>\n",
              "      <td>148.722226</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>247.400000</td>\n",
              "      <td>0.088889</td>\n",
              "      <td>138.058466</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>219.600000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.844444</td>\n",
              "      <td>0.319109</td>\n",
              "      <td>0.311111</td>\n",
              "      <td>0.421504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.066667</td>\n",
              "      <td>0.676731</td>\n",
              "      <td>171.088892</td>\n",
              "      <td>81.400000</td>\n",
              "      <td>256.000000</td>\n",
              "      <td>0.144444</td>\n",
              "      <td>156.228848</td>\n",
              "      <td>81.400000</td>\n",
              "      <td>227.400000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.400642</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.435727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.300000</td>\n",
              "      <td>0.653520</td>\n",
              "      <td>158.100000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>254.200000</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>150.563724</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>233.000000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.300330</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.500892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>0.691043</td>\n",
              "      <td>162.833334</td>\n",
              "      <td>71.400000</td>\n",
              "      <td>255.200000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>143.939212</td>\n",
              "      <td>71.400000</td>\n",
              "      <td>241.000000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.317054</td>\n",
              "      <td>0.366667</td>\n",
              "      <td>0.489206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.288889</td>\n",
              "      <td>0.612695</td>\n",
              "      <td>150.799998</td>\n",
              "      <td>87.400000</td>\n",
              "      <td>238.000000</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>145.503334</td>\n",
              "      <td>87.400000</td>\n",
              "      <td>234.800000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.911111</td>\n",
              "      <td>0.244516</td>\n",
              "      <td>0.377778</td>\n",
              "      <td>0.485159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.322222</td>\n",
              "      <td>0.609334</td>\n",
              "      <td>149.455560</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>250.400000</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>141.423721</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>246.200000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.922222</td>\n",
              "      <td>0.197375</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.499516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.244444</td>\n",
              "      <td>0.658883</td>\n",
              "      <td>153.088892</td>\n",
              "      <td>74.800000</td>\n",
              "      <td>246.400000</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>141.215944</td>\n",
              "      <td>74.800000</td>\n",
              "      <td>220.200000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.877778</td>\n",
              "      <td>0.321212</td>\n",
              "      <td>0.366667</td>\n",
              "      <td>0.489432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.311111</td>\n",
              "      <td>0.633232</td>\n",
              "      <td>137.055556</td>\n",
              "      <td>65.800000</td>\n",
              "      <td>246.200000</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>130.048857</td>\n",
              "      <td>65.800000</td>\n",
              "      <td>203.800000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.911111</td>\n",
              "      <td>0.288309</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.492416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.211111</td>\n",
              "      <td>0.671328</td>\n",
              "      <td>148.500000</td>\n",
              "      <td>69.600000</td>\n",
              "      <td>256.000000</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>132.250589</td>\n",
              "      <td>69.600000</td>\n",
              "      <td>217.800000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.855556</td>\n",
              "      <td>0.350768</td>\n",
              "      <td>0.355556</td>\n",
              "      <td>0.469853</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training complete.\n",
            "TrainOutput(global_step=100, training_loss=1.536249492062325e-08, metrics={'train_runtime': 3629.7756, 'train_samples_per_second': 0.496, 'train_steps_per_second': 0.028, 'total_flos': 0.0, 'train_loss': 1.536249492062325e-08})\n",
            "Wall clock (s): 3633.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell H â€” Evaluate GRPO model on GSM8K (exact-match of <answer> tag)\n",
        "\n",
        "What this cell does:\n",
        "1) Switches the model to fast inference and eval mode.\n",
        "2) Runs greedy generation (do_sample=False) on N_EVAL prompts.\n",
        "3) Extracts the numeric string inside <answer>...</answer>.\n",
        "4) Computes exact-match accuracy against our `ground_truth`.\n",
        "5) Prints a few qualitative examples (Q, predicted, truth).\n",
        "\n",
        "Why this is correct:\n",
        "- GSM8K evaluation usually checks the final numeric answer only.\n",
        "- Our GRPO format enforces:\n",
        "      <reasoning>...</reasoning>\n",
        "      <answer>...</answer>\n",
        "  so we parse the answer tag directly.\n",
        "- We use Transformers' chat templating for consistent formatting.\n",
        "\"\"\"\n",
        "\n",
        "import re, math, torch, random, time\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# 1) Enable fast inference & eval mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "model.eval()\n",
        "\n",
        "# 2) Config\n",
        "N_EVAL = min(64, len(train))\n",
        "MAX_NEW_TOKENS = 256\n",
        "ANSWER_RE = re.compile(\n",
        "    r\"<answer>\\s*([\\s\\S]+?)\\s*</answer>\", re.IGNORECASE\n",
        ")\n",
        "NUM_NORM = re.compile(r\"^\\s*([-+]?\\d[\\d,]*([.]\\d+)?)\\s*$\")\n",
        "\n",
        "def _normalize_num(s: str | None):\n",
        "    if not s: return None\n",
        "    s = s.strip()\n",
        "    m = NUM_NORM.match(s)\n",
        "    if m:\n",
        "        s = m.group(1)\n",
        "    return s.replace(\",\", \"\").strip()\n",
        "\n",
        "def generate_one(messages):\n",
        "    # Return decoded new text only (post prompt)\n",
        "    enc = tokenizer.apply_chat_template(\n",
        "        messages, add_generation_prompt=True,\n",
        "        return_tensors=\"pt\", return_dict=True, tokenize=True\n",
        "    )\n",
        "    # Move to device\n",
        "    enc = {k: (v.to(model.device) if hasattr(v,\"to\") else v) for k,v in enc.items()}\n",
        "    prompt_len = enc[\"input_ids\"].shape[1]\n",
        "    with torch.no_grad(), torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
        "        out = model.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            do_sample=False,                # greedy for deterministic eval\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    new_tokens = out[0, prompt_len:]\n",
        "    return tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "correct = 0\n",
        "examples = []\n",
        "t0 = time.time()\n",
        "\n",
        "# 3) Evaluate first N_EVAL rows (you can randomize if you prefer)\n",
        "for i in range(N_EVAL):\n",
        "    row = train[i]\n",
        "    pred_text = generate_one(row[\"prompt\"])\n",
        "    # Parse <answer>...</answer>\n",
        "    m = ANSWER_RE.search(pred_text)\n",
        "    pred = _normalize_num(m.group(1) if m else None)\n",
        "    gold = _normalize_num(row[\"ground_truth\"])\n",
        "    ok = (pred is not None and gold is not None and pred == gold)\n",
        "    correct += int(ok)\n",
        "    if len(examples) < 5:\n",
        "        examples.append({\n",
        "            \"q\": row[\"question\"][:140] + (\"â€¦\" if len(row[\"question\"])>140 else \"\"),\n",
        "            \"pred\": (pred if pred is not None else \"âˆ…\"),\n",
        "            \"gold\": gold,\n",
        "            \"ok\": ok,\n",
        "        })\n",
        "\n",
        "acc = correct / N_EVAL if N_EVAL else 0.0\n",
        "t1 = time.time()\n",
        "\n",
        "print(f\"Evaluated {N_EVAL} prompts.\")\n",
        "print(f\"Exact-match accuracy: {acc:.3f}\")\n",
        "print(\"Examples:\")\n",
        "for ex in examples:\n",
        "    print(f\"â€¢ ok={ex['ok']} | pred={ex['pred']} | gold={ex['gold']} | Q: {ex['q']}\")\n",
        "print(f\"Wall clock: {t1 - t0:.1f}s\")\n",
        "print(\"âœ… Eval complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT9rRokkwO0D",
        "outputId": "e2a4c70d-4c61-46ca-e340-603fe8c403de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated 64 prompts.\n",
            "Exact-match accuracy: 0.281\n",
            "Examples:\n",
            "â€¢ ok=False | pred=36 | gold=72 | Q: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether inâ€¦\n",
            "â€¢ ok=False | pred=\\$10 | gold=10 | Q: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
            "â€¢ ok=False | pred=$5 | gold=5 | Q: Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 forâ€¦\n",
            "â€¢ ok=True | pred=42 | gold=42 | Q: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wantâ€¦\n",
            "â€¢ ok=False | pred=416 | gold=624 | Q: James writes a 3-page letter to 2 different friends twice a week.  How many pages does he write a year?\n",
            "Wall clock: 690.2s\n",
            "âœ… Eval complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell I â€” Deploy: fast inference helper + sample generations + save LoRA adapters\n",
        "\n",
        "What this cell does:\n",
        "1) Switches the model to Unslothâ€™s fast inference path.\n",
        "2) Defines a robust `chat()` that uses the tokenizerâ€™s chat template and works with PEFT models.\n",
        "3) Runs a couple of sample generations (one from GSM8K, one custom).\n",
        "4) Saves your LoRA adapters + tokenizer files for reuse or serving.\n",
        "\n",
        "Why this is correct:\n",
        "- Unslothâ€™s docs recommend `FastLanguageModel.for_inference(model)` for faster inference and show saving\n",
        "  adapters with `save_pretrained`.  (We keep the chat template path from training.)\n",
        "- Transformersâ€™ chat templating (`apply_chat_template`) is the supported way to format prompts for chat models.\n",
        "References: Unsloth Inference & Running/Saving docs; HF chat templating docs; TRL GRPO reward/inference conventions.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from collections.abc import Mapping\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# 1) Fast inference path\n",
        "FastLanguageModel.for_inference(model)\n",
        "model.eval()\n",
        "\n",
        "def _to_device(batch, device):\n",
        "    if isinstance(batch, Mapping):\n",
        "        return {k: (v.to(device) if hasattr(v, \"to\") else v) for k, v in batch.items()}\n",
        "    if torch.is_tensor(batch):\n",
        "        return {\"input_ids\": batch.to(device),\n",
        "                \"attention_mask\": torch.ones_like(batch, device=device)}\n",
        "    raise TypeError(f\"Unexpected inputs type: {type(batch)}\")\n",
        "\n",
        "def chat(messages, max_new_tokens=256, temperature=None, top_p=None):\n",
        "    \"\"\"\n",
        "    Format messages with the tokenizer's chat template and generate a reply.\n",
        "    If `temperature` is None, we do greedy decoding (deterministic); otherwise we sample.\n",
        "    Returns only the newly generated text after the prompt.\n",
        "    \"\"\"\n",
        "    enc = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_dict=True,\n",
        "        tokenize=True,\n",
        "    )\n",
        "    enc = _to_device(enc, model.device)\n",
        "    prompt_len = enc[\"input_ids\"].shape[1]\n",
        "\n",
        "    gen_kwargs = dict(\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "    if temperature is None:\n",
        "        gen_kwargs.update(do_sample=False)\n",
        "    else:\n",
        "        gen_kwargs.update(do_sample=True, temperature=float(temperature), top_p=float(top_p or 0.95))\n",
        "\n",
        "    with torch.no_grad(), torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
        "        out = model.generate(**enc, **gen_kwargs)\n",
        "\n",
        "    new_tokens = out[0, prompt_len:]\n",
        "    return tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "# 2) Sample generations\n",
        "SYSTEM_PROMPT = (\n",
        "    \"Respond in the following format:\\n\"\n",
        "    \"<reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\"\n",
        ")\n",
        "\n",
        "# (A) Reuse one GSM8K question to verify tags/format\n",
        "ex_user = train[0][\"question\"]\n",
        "print(\"=== Example A (GSM8K) â€” greedy ===\")\n",
        "print(chat([{\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
        "            {\"role\":\"user\",\"content\":ex_user}], max_new_tokens=256))\n",
        "\n",
        "# (B) Custom math prompt (sampled)\n",
        "print(\"\\n=== Example B (custom) â€” sampled ===\")\n",
        "print(chat([{\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
        "            {\"role\":\"user\",\"content\":\"A bag has 6 red and 4 blue marbles. If I draw 3 without replacement, what is the probability all are red?\"}],\n",
        "            temperature=0.8, top_p=0.95, max_new_tokens=256))\n",
        "\n",
        "# 3) Save LoRA adapters + tokenizer\n",
        "SAVE_DIR = \"grpo_qwen15b_lora\"\n",
        "model.save_pretrained(SAVE_DIR)\n",
        "tokenizer.save_pretrained(SAVE_DIR)\n",
        "print(f\"\\nâœ… Saved LoRA adapters & tokenizer to: {SAVE_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEE8bJBw-bYf",
        "outputId": "f99f280d-4c84-44c5-9b6e-815e4120fbcd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Example A (GSM8K) â€” greedy ===\n",
            "<reasoning>\n",
            "To find out how many clips Natalia sold altogether in April and May, we need to follow these steps:\n",
            "\n",
            "1. Determine the number of clips sold in May.\n",
            "2. Add the number of clips sold in April to the number of clips sold in May.\n",
            "\n",
            "Given that Natalia sold half as many clips in May as she did in April, we can calculate the number of clips sold in May by dividing the number of clips sold in April by 2.\n",
            "\n",
            "Let's perform this calculation.\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "36\n",
            "</answer>\n",
            "\n",
            "=== Example B (custom) â€” sampled ===\n",
            "<reasoning>\n",
            "To find the probability that all three drawn marbles are red, we need to consider the total number of ways to choose 3 marbles from the bag and divide it by the number of ways to choose 3 red marbles.\n",
            "The total number of ways to choose 3 marbles from a bag with 10 marbles (6 red and 4 blue) can be calculated using combinations: $C(10,3)$.\n",
            "\n",
            "Similarly, the number of ways to choose 3 red marbles from 6 red marbles can also be calculated using combinations: $C(6,3)$.\n",
            "Therefore, the probability that all three drawn marbles are red would be the ratio of these two values.\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "$$\\frac{C(6,3)}{C(10,3)}$$\n",
            "</answer>\n",
            "\n",
            "âœ… Saved LoRA adapters & tokenizer to: grpo_qwen15b_lora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8s99GwCUBfd2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}