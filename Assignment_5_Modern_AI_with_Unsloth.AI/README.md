# Modern AI with Unsloth.ai
Note - If the ipynb files do not open in github, use the colab links from readme.
This assignment demonstrates **modern fine-tuning, reinforcement learning, and continued pretraining techniques** using [Unsloth.ai](https://unsloth.ai).  
It includes **five Colab notebooks**, each focusing on a different method for training and optimizing LLMs efficiently.

---

## üìò Colab Notebooks Overview

### üß© Colab 1 ‚Äì Full Fine-Tuning with Small Model  
**Description:**  
Perform **full fine-tuning** using the **smollm2-135M** model (`full_finetuning=True`).  
This notebook demonstrates end-to-end training on a chosen task (e.g., **chat** or **code generation**) using **open-weights LLMs** such as:  
- `unsloth/gemma-3-1b-it-unsloth-bnb-4bit`  
- Llama 3.1 (8B), Mistral NeMo (12B), Gemma 2 (9B), Phi-3.5 (mini), Qwen2 (7B), TinyLlama, and others  

You will explain in the video:
- How Unsloth fine-tuning works  
- Input formats and datasets used  
- Application of chat model templates  

**Colab Link:** [Open in Colab](https://colab.research.google.com/drive/1Gu4uRRwpeWpaeF6PQplGgcCTBjSySy77?usp=sharing)  
**References:**  
- [Fine-tuning LLMs Guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide)  
- [Unsloth + Ollama Lora Tutorial](https://sarinsuriyakoon.medium.com/unsloth-lora-with-ollama-lightweight-solution-to-full-cycle-llm-development-edadb6d9e0f0)

---

### ‚öôÔ∏è Colab 2 ‚Äì LoRA Parameter-Efficient Fine-Tuning  
**Description:**  
Repeat the same fine-tuning task as Colab 1, but with **LoRA (Low-Rank Adaptation)** for parameter-efficient training.  
Only adapter weights are updated, enabling faster, memory-efficient training while achieving similar performance.  

**Colab Link:** [Open in Colab](https://colab.research.google.com/drive/1UglSIij88szHDBc7t9gtRjYgL7ltTs3t?usp=sharing)  
**Reference:**  
- [Fine-tuning LLMs Guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide)

---

### üß† Colab 3 ‚Äì Reinforcement Learning with Preference Data  
**Description:**  
Use a dataset containing **input**, **preferred output**, and **rejected output** pairs to perform **Reinforcement Learning (RL)** fine-tuning.  
Demonstrates how Unsloth supports preference-based learning for improving model alignment.

**Colab Link:** [Open in Colab](https://colab.research.google.com/drive/1IsB4PiBU5Y_iolIsJEP893TDOZBrvqya?usp=sharing)  
**Reference:**  
- [Unsloth Reinforcement Learning Guide](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide)

---

### üß© Colab 4 ‚Äì Reinforcement Learning with GRPO  
**Description:**  
Train a **reasoning-capable LLM** using **GRPO (Generative Reinforcement Policy Optimization)**.  
Dataset includes problem statements, and answers are generated by the LLM itself.  
You‚Äôll experiment with reasoning models as detailed in:  
- [Train Your Own Reasoning Model with GRPO](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/tutorial-train-your-own-reasoning-model-with-grpo)  
- [Unsloth R1 Reasoning Blog](https://unsloth.ai/blog/r1-reasoning)

**Colab Link:** [Open in Colab](https://colab.research.google.com/drive/1_H9prFujz7xMX3NzECyJznNb5-qPWhDa?usp=sharing)

---

### üåç Colab 5 ‚Äì Continued Pretraining  
**Description:**  
Perform **continued pretraining** to teach an LLM a **new language** or domain using Unsloth‚Äôs efficient pretraining framework.  
Show how model knowledge expands through incremental training.

**Colab Link:** [Open in Colab](https://colab.research.google.com/drive/1Xbv0MG4OYbD0HqJ2dHK0LNy7-zUaBPZS?usp=sharing)  
**Reference:**  
- [Continued Pretraining Guide](https://docs.unsloth.ai/basics/continued-pretraining)

---

## üé• Video Walkthrough  
**End-to-End Explanation and Code Walkthrough**  
[Watch Here - Colab 1](https://youtu.be/u7RBlY8DrlQ)

[Watch Here - Colab 2](https://youtu.be/UZtrQa9Qh9Y)

[Watch Here - Colab 3](https://youtu.be/ljQHaPl3Pqk)

[Watch Here - Colab 4](https://youtu.be/2AoFGv_uGHg)

[Watch Here - Colab 5](https://youtu.be/wDImkF9tBmI)

