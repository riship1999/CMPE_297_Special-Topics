# Modern AI with Unsloth.ai

This assignment demonstrates **modern fine-tuning, reinforcement learning, and continued pretraining techniques** using [Unsloth.ai](https://unsloth.ai).  
It includes **five Colab notebooks**, each focusing on a different method for training and optimizing LLMs efficiently.

---

## üìò Colab Notebooks Overview

### üß© Colab 1 ‚Äì Full Fine-Tuning with Small Model  
**Description:**  
Perform **full fine-tuning** using the **smollm2-135M** model (`full_finetuning=True`).  
This notebook demonstrates end-to-end training on a chosen task (e.g., **chat** or **code generation**) using **open-weights LLMs** such as:  
- `unsloth/gemma-3-1b-it-unsloth-bnb-4bit`  
- Llama 3.1 (8B), Mistral NeMo (12B), Gemma 2 (9B), Phi-3.5 (mini), Qwen2 (7B), TinyLlama, and others  

You will explain in the video:
- How Unsloth fine-tuning works  
- Input formats and datasets used  
- Application of chat model templates  

**Colab Link:** [Open in Colab](#)  
**References:**  
- [Fine-tuning LLMs Guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide)  
- [Unsloth + Ollama Lora Tutorial](https://sarinsuriyakoon.medium.com/unsloth-lora-with-ollama-lightweight-solution-to-full-cycle-llm-development-edadb6d9e0f0)

---

### ‚öôÔ∏è Colab 2 ‚Äì LoRA Parameter-Efficient Fine-Tuning  
**Description:**  
Repeat the same fine-tuning task as Colab 1, but with **LoRA (Low-Rank Adaptation)** for parameter-efficient training.  
Only adapter weights are updated, enabling faster, memory-efficient training while achieving similar performance.  

**Colab Link:** [Open in Colab](#)  
**Reference:**  
- [Fine-tuning LLMs Guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide)

---

### üß† Colab 3 ‚Äì Reinforcement Learning with Preference Data  
**Description:**  
Use a dataset containing **input**, **preferred output**, and **rejected output** pairs to perform **Reinforcement Learning (RL)** fine-tuning.  
Demonstrates how Unsloth supports preference-based learning for improving model alignment.

**Colab Link:** [Open in Colab](#)  
**Reference:**  
- [Unsloth Reinforcement Learning Guide](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide)

---

### üß© Colab 4 ‚Äì Reinforcement Learning with GRPO  
**Description:**  
Train a **reasoning-capable LLM** using **GRPO (Generative Reinforcement Policy Optimization)**.  
Dataset includes problem statements, and answers are generated by the LLM itself.  
You‚Äôll experiment with reasoning models as detailed in:  
- [Train Your Own Reasoning Model with GRPO](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/tutorial-train-your-own-reasoning-model-with-grpo)  
- [Unsloth R1 Reasoning Blog](https://unsloth.ai/blog/r1-reasoning)

**Colab Link:** [Open in Colab](#)

---

### üåç Colab 5 ‚Äì Continued Pretraining  
**Description:**  
Perform **continued pretraining** to teach an LLM a **new language** or domain using Unsloth‚Äôs efficient pretraining framework.  
Show how model knowledge expands through incremental training.

**Colab Link:** [Open in Colab](#)  
**Reference:**  
- [Continued Pretraining Guide](https://docs.unsloth.ai/basics/continued-pretraining)

---

## üé• Video Walkthrough  
**End-to-End Explanation and Code Walkthrough**  
[Watch Here](#)


